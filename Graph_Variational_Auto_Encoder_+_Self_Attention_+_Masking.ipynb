{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Graph Variational Auto Encoder + Self Attention + Masking.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZYWajO4kv8yz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPcWQJgolD2b"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2Bzxm_cSbK0"
      },
      "source": [
        "! pip install rdkit-pypi\n",
        "! pip install deepchem\n",
        "! pip install dgl \n",
        "! pip install ogb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qsdRbKWE1Pr"
      },
      "source": [
        "## to resolve the torch import error\n",
        "# ! pip install -U numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm-TP2wbS6fB"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNfhiFiBS8-w"
      },
      "source": [
        "import numpy as np\n",
        "import pandas\n",
        "import time\n",
        "import networkx as nx\n",
        "import itertools\n",
        "import scipy.sparse as sp\n",
        "import random "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjTrLz-8WeV6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_T2Je7g0_lT"
      },
      "source": [
        "from rdkit.Chem import MACCSkeys\n",
        "from rdkit import Chem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPe8AThO1RC8"
      },
      "source": [
        "import dgl\n",
        "from dgl.nn import SAGEConv,GraphConv\n",
        "import dgl.function as fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWJMti881g5R"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from ogb.linkproppred import Evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiWD2YpW-tK0"
      },
      "source": [
        "import copy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ELNvAwZLkG"
      },
      "source": [
        "from ogb.utils.features import (allowable_features, atom_to_feature_vector,\n",
        " bond_to_feature_vector, atom_feature_vector_to_dict, bond_feature_vector_to_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9KaG8LslK3w"
      },
      "source": [
        "## Read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RAisVRmBwcy"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/r-b-1-5/Public-files/main/drugIDandSMILES.csv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-17T05:18:44.102582Z",
          "iopub.status.busy": "2021-09-17T05:18:44.101768Z",
          "iopub.status.idle": "2021-09-17T05:18:44.147656Z",
          "shell.execute_reply": "2021-09-17T05:18:44.148290Z",
          "shell.execute_reply.started": "2021-09-17T04:52:16.490607Z"
        },
        "papermill": {
          "duration": 0.095447,
          "end_time": "2021-09-17T05:18:44.148475",
          "exception": false,
          "start_time": "2021-09-17T05:18:44.053028",
          "status": "completed"
        },
        "tags": [],
        "id": "686e079f"
      },
      "source": [
        "csvFile = pandas.read_csv('./drugIDandSMILES.csv')\n",
        " \n",
        "print(len(csvFile))\n",
        "print(csvFile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-17T05:18:44.244612Z",
          "iopub.status.busy": "2021-09-17T05:18:44.239607Z",
          "iopub.status.idle": "2021-09-17T05:18:44.247246Z",
          "shell.execute_reply": "2021-09-17T05:18:44.247694Z",
          "shell.execute_reply.started": "2021-09-17T04:52:19.348934Z"
        },
        "papermill": {
          "duration": 0.055187,
          "end_time": "2021-09-17T05:18:44.247895",
          "exception": false,
          "start_time": "2021-09-17T05:18:44.192708",
          "status": "completed"
        },
        "tags": [],
        "id": "b033cccc"
      },
      "source": [
        "drug_id = csvFile['Drug ID']\n",
        "smiles = csvFile['SMILES']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYWajO4kv8yz"
      },
      "source": [
        "## Globals\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDgFu463v9Gb"
      },
      "source": [
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGztPnrQlNQC"
      },
      "source": [
        "## Form graph from molecule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3n2JxSxa-Mez"
      },
      "source": [
        "def smiles2graph(smiles_string):\n",
        "\n",
        "    mol = Chem.MolFromSmiles(smiles_string)\n",
        "\n",
        "    try:\n",
        "        A = Chem.GetAdjacencyMatrix(mol)\n",
        "        A = np.asmatrix(A)\n",
        "        nnodes=len(A)\n",
        "        nz = np.nonzero(A)\n",
        "    except:\n",
        "        return dgl.graph()\n",
        "    # forming the graph using the adjacency matrix\n",
        "    u1, v1 = list(nz[0]), list(nz[1])\n",
        "    # print(sorted(u1)==sorted(v1))\n",
        "    # print(u1)\n",
        "    # print(v1)\n",
        "    g = dgl.graph((u1, v1))\n",
        "    bg = dgl.to_bidirected(g)\n",
        "\n",
        "    # # atoms\n",
        "    atom_features_list = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        atom_features_list.append(atom_to_feature_vector(atom))\n",
        "    x = np.array(atom_features_list, dtype = np.int64)\n",
        "\n",
        "    # # bonds\n",
        "    # num_bond_features = 3  # bond type, bond stereo, is_conjugated\n",
        "    # if len(mol.GetBonds()) > 0: # mol has bonds\n",
        "    #     edges_list = []\n",
        "    #     edge_features_list = []\n",
        "    #     for bond in mol.GetBonds():\n",
        "    #         i = bond.GetBeginAtomIdx()\n",
        "    #         j = bond.GetEndAtomIdx()\n",
        "\n",
        "    #         edge_feature = bond_to_feature_vector(bond)\n",
        "\n",
        "    #         # add edges in both directions\n",
        "    #         edges_list.append((i, j))\n",
        "    #         edge_features_list.append(edge_feature)\n",
        "    #         edges_list.append((j, i))\n",
        "    #         edge_features_list.append(edge_feature)\n",
        "\n",
        "    #     # data.edge_index: Graph connectivity in COO format with shape [2, num_edges]\n",
        "    #     edge_index = np.array(edges_list, dtype = np.int64).T\n",
        "\n",
        "    #     # data.edge_attr: Edge feature matrix with shape [num_edges, num_edge_features]\n",
        "    #     edge_attr = np.array(edge_features_list, dtype = np.int64)\n",
        "\n",
        "    # else:   # mol has no bonds\n",
        "    #     edge_index = np.empty((2, 0), dtype = np.int64)\n",
        "    #     edge_attr = np.empty((0, num_bond_features), dtype = np.int64)\n",
        "\n",
        "    # print(edge_attr.shape, edge_index.shape, x.shape)\n",
        "    bg.ndata['node_feat'] = torch.FloatTensor(x)\n",
        "    # bg.edata['edge_feat'] = torch.tensor(edge_attr)\n",
        "\n",
        "    # return graph \n",
        "    return bg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvrll-n0DbGn"
      },
      "source": [
        "mol1 = smiles2graph(smiles[0])\n",
        "print(mol1)\n",
        "mol2 = smiles2graph(smiles[1])\n",
        "print(mol2)\n",
        "# print(mol1.ndata['node_feat'])\n",
        "# print(mol2.ndata['node_feat'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MA8k3pHrKxa"
      },
      "source": [
        "help(atom_to_feature_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn_M_ND-LZfs"
      },
      "source": [
        "# https://github.com/snap-stanford/ogb/blob/master/ogb/utils/features.py\n",
        "maxnodes = 0\n",
        "exception_count = 0\n",
        "for _ in range(len(smiles)):\n",
        "  try:\n",
        "      mol_ = smiles2graph(smiles[_])\n",
        "      maxnodes = max(maxnodes, mol_.num_nodes())\n",
        "  except:\n",
        "      exception_count += 1\n",
        "      # print(smiles[_])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBXvI40BLZl6"
      },
      "source": [
        "print(maxnodes)\n",
        "print(exception_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCsBbYcW2X5r"
      },
      "source": [
        "# https://github.com/snap-stanford/ogb/blob/master/ogb/utils/features.py#L3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pG2e8nnTDGH"
      },
      "source": [
        "## VAE for Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZ7kvPKWTFfE"
      },
      "source": [
        "print(maxnodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JF-R94aTv_s"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layersList = nn.ModuleList()\n",
        "        self.layersList.append(SAGEConv(in_feats, out_feats, 'gcn'))\n",
        "        self.layersList.append(SAGEConv(out_feats, out_feats, 'gcn'))\n",
        "        self.attention = Attention(maxnodes,maxnodes)\n",
        "        self.fc = nn.Linear( maxnodes*out_feats , 100)\n",
        "        self.fc_mu = nn.Linear( 100 , 30)\n",
        "        self.fc_var = nn.Linear( 100 , 30)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, g, feats):\n",
        "        temp = feats\n",
        "        for L in self.layersList:\n",
        "            temp = L(g,temp)\n",
        "            temp = self.relu(temp)\n",
        "        temp2 = temp\n",
        "        dim1 = temp.shape[0]\n",
        "        if maxnodes != dim1:\n",
        "            padder = torch.zeros(maxnodes-dim1,3)\n",
        "            temp = torch.cat([temp, padder], dim = 0)\n",
        "        temp = self.attention(temp.t()).t()\n",
        "        temp = torch.flatten(temp)\n",
        "        temp = self.relu(self.fc(temp))\n",
        "        mu = self.fc_mu(temp)\n",
        "        # var = torch.exp(self.fc_var(temp))\n",
        "        log_var = self.fc_var(temp)\n",
        "        # return temp2, temp, mu, log_var\n",
        "        return temp2, mu, log_var\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_feat,out_feat):\n",
        "        super().__init__()             \n",
        "        self.Q = nn.Linear(in_feat,out_feat) # Query\n",
        "        self.K = nn.Linear(in_feat,out_feat) # Key\n",
        "        self.V = nn.Linear(in_feat,out_feat) # Value\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        Q = self.Q(x)\n",
        "        K = self.K(x)\n",
        "        V = self.V(x)\n",
        "        d = K.shape[0] # dimension of key vector\n",
        "        QK_d = (Q @ K.T)/(d)**0.5\n",
        "        prob = self.softmax(QK_d)\n",
        "        attention = prob @ V\n",
        "        return attention\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layersList = nn.ModuleList()\n",
        "        self.layersList.append(SAGEConv(in_feats, out_feats, 'gcn'))\n",
        "        self.layersList.append(SAGEConv(out_feats, out_feats, 'gcn'))\n",
        "        self.fc1 = nn.Linear( 30, 100)\n",
        "        self.fc2 = nn.Linear( 100, maxnodes*in_feats)\n",
        "        # self.attention = Attention(maxnodes, maxnodes)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, g, res, hidden):\n",
        "        temp = self.fc1(hidden)\n",
        "        temp = self.fc2(temp)\n",
        "        temp = torch.reshape(temp, (maxnodes, 3))\n",
        "        # temp = self.attention(temp.t()).t()\n",
        "        temp = temp[:res.shape[0]]\n",
        "        temp += res\n",
        "        for L in self.layersList:\n",
        "            temp = L(g,temp)\n",
        "            temp = self.relu(temp)\n",
        "        return temp\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder(in_feats, out_feats)\n",
        "        self.decoder = Decoder(out_feats, in_feats)\n",
        "        self.batchNormLayer = nn.BatchNorm1d(out_feats)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        # sampling\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        # temp, encoded, mu, log_var = self.encoder(g, inputs)\n",
        "        temp, mu, log_var = self.encoder(g, inputs)\n",
        "        hidden = self.reparameterize(mu, log_var)\n",
        "        temp = self.batchNormLayer(temp)\n",
        "        temp = self.decoder(g, temp, hidden)\n",
        "        return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7UXBH2spfmj"
      },
      "source": [
        "set_seed(0)\n",
        "g = smiles2graph(smiles[0])\n",
        "g = dgl.add_self_loop(g)\n",
        "\n",
        "model = VariationalAutoEncoder(g.ndata['node_feat'].shape[1], 3)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# loss_fn = nn.KLDivLoss() + nn.MSELoss()\n",
        "loss_fn1 = nn.KLDivLoss() \n",
        "loss_fn2 = nn.MSELoss()\n",
        "\n",
        "all_logits = []\n",
        "for e in range(1000):\n",
        "    pred = model(g, g.ndata['node_feat'])\n",
        "    pred = F.log_softmax(pred, 1)\n",
        "    loss = loss_fn1(pred, g.ndata['node_feat']) + loss_fn2(pred, g.ndata['node_feat'])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if e % 50 == 0:\n",
        "        print('In epoch {}, loss: {}'.format(e, loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X34H33HSpKG8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ERUxVzmCWpF"
      },
      "source": [
        "## VAE Padding using Mask(Working)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Bvd0HCCWpW"
      },
      "source": [
        "print(maxnodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo8L4Tw_CWpW"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layersList = nn.ModuleList()\n",
        "        self.layersList.append(SAGEConv(in_feats, out_feats, 'gcn'))\n",
        "        self.layersList.append(SAGEConv(out_feats, out_feats, 'gcn'))\n",
        "        self.attention = Attention(maxnodes,maxnodes)\n",
        "        self.fc = nn.Linear(maxnodes*out_feats , 100)\n",
        "        self.fc_mu = nn.Linear( 100 , 30)\n",
        "        self.fc_var = nn.Linear( 100 , 30)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, g, feats):\n",
        "        temp = feats\n",
        "        # print(temp.shape)\n",
        "        for L in self.layersList:\n",
        "            temp = L(g,temp)\n",
        "            temp = self.relu(temp)\n",
        "        temp2 = temp\n",
        "        # print(\"e1\")\n",
        "        dim1 = temp.shape[0]\n",
        "        if maxnodes != dim1:\n",
        "            padder = torch.empty(maxnodes-dim1,3).fill_(-0.03)\n",
        "            temp = torch.cat([temp, padder], dim = 0)\n",
        "        mask = self.make_input_mask(temp)\n",
        "        # print(mask)\n",
        "        # print(temp.shape, mask.shape) # --> 551 * 9\n",
        "        temp = self.attention(temp.t(), mask.t()).t()\n",
        "        temp = torch.flatten(temp)\n",
        "        # print(\"e2\")\n",
        "        temp = self.relu(self.fc(temp))\n",
        "        mu = self.fc_mu(temp)\n",
        "        # var = torch.exp(self.fc_var(temp))\n",
        "        log_var = self.fc_var(temp)\n",
        "        # return temp2, temp, mu, log_var\n",
        "        return temp2, mu, log_var\n",
        "\n",
        "    def make_input_mask(self, src):\n",
        "        # using -100 for the padded indices\n",
        "        src_mask = (src != -0.03)\n",
        "        # print(src_mask.shape) # 551 * 9\n",
        "        return src_mask\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_feat,out_feat):\n",
        "        super().__init__()             \n",
        "        self.Q = nn.Linear(in_feat,out_feat) # Query\n",
        "        self.K = nn.Linear(in_feat,out_feat) # Key\n",
        "        self.V = nn.Linear(in_feat,out_feat) # Value\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        Q = self.Q(x)\n",
        "        K = self.K(x)\n",
        "        V = self.V(x)\n",
        "        d = K.shape[0] # dimension of key vector\n",
        "        QK_d = (Q @ K.T)/((d)**0.5)\n",
        "        # print(QK_d.shape) # --> 551 * 551 \n",
        "        prob = self.softmax(QK_d)\n",
        "        attention = prob @ V\n",
        "        # print(attention.shape) --> 551 * 9\n",
        "        # print(\"sa1\")\n",
        "        ######\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, 0)\n",
        "        ######\n",
        "        # all zeros in the padded layers\n",
        "        # print(attention[])\n",
        "        return attention\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layersList = nn.ModuleList()\n",
        "        self.layersList.append(SAGEConv(in_feats, out_feats, 'gcn'))\n",
        "        self.layersList.append(SAGEConv(out_feats, out_feats, 'gcn'))\n",
        "        self.fc1 = nn.Linear( 30, 100)\n",
        "        self.fc2 = nn.Linear( 100, maxnodes*in_feats)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, g, res, hidden):\n",
        "        temp = self.fc1(hidden)\n",
        "        temp = self.fc2(temp)\n",
        "        temp = torch.reshape(temp, (maxnodes, 3))\n",
        "        temp = temp[:res.shape[0]]\n",
        "        temp += res\n",
        "        for L in self.layersList:\n",
        "            temp = L(g,temp)\n",
        "            temp = self.relu(temp)\n",
        "        return temp\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder(in_feats, out_feats)\n",
        "        self.decoder = Decoder(out_feats, in_feats)\n",
        "        self.batchNormLayer = nn.BatchNorm1d(out_feats)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        # sampling\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        # temp, encoded, mu, log_var = self.encoder(g, inputs)\n",
        "        # print(\"1\")\n",
        "        # print(inputs.shape, input_mask.shape) --> 551 *9 \n",
        "        temp, mu, log_var = self.encoder(g, inputs)\n",
        "        # print(\"2\")\n",
        "        hidden = self.reparameterize(mu, log_var)\n",
        "        # print(\"3\")\n",
        "        temp = self.batchNormLayer(temp)\n",
        "        # print(\"4\")\n",
        "        temp = self.decoder(g, temp, hidden)\n",
        "        # print(\"5\")\n",
        "        return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I_0W73iCWpY"
      },
      "source": [
        "set_seed(0)\n",
        "g = smiles2graph(smiles[0])\n",
        "g = dgl.add_self_loop(g)\n",
        "\n",
        "model = VariationalAutoEncoder(g.ndata['node_feat'].shape[1], 3)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "loss_fn1 = nn.KLDivLoss() \n",
        "loss_fn2 = nn.MSELoss()\n",
        "\n",
        "all_logits = []\n",
        "for e in range(1000):\n",
        "    pred = model(g, g.ndata['node_feat'])\n",
        "    pred = F.log_softmax(pred, 1)\n",
        "    loss = loss_fn1(pred, g.ndata['node_feat']) + loss_fn2(pred, g.ndata['node_feat'])\n",
        "    # loss = loss_fn1(pred, g.ndata['node_feat'])\n",
        "    # loss = loss_fn2(pred, g.ndata['node_feat'])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if e % 50 == 0:\n",
        "        print('In epoch {}, loss: {}'.format(e, loss))\n",
        "\n",
        "    # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qV8fzwEPOB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyp-y0R0GFbY"
      },
      "source": [
        "## VAE for Graph(BATCH) (INCOMPLETE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8FoqgwBGFba"
      },
      "source": [
        "print(maxnodes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2b4PPvgZzoH"
      },
      "source": [
        "print(drug_id)\n",
        "print(smiles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VId-YCeWdjkK"
      },
      "source": [
        "from rdkit.Chem import SaltRemover\n",
        "\n",
        "def smiles2graphNew(smiles_string):\n",
        "\n",
        "    mol = Chem.MolFromSmiles(smiles_string)\n",
        "    mol = SaltRemover.StripMol(mol)\n",
        "\n",
        "    try:\n",
        "        A = Chem.GetAdjacencyMatrix(mol)\n",
        "        A = np.asmatrix(A)\n",
        "        nnodes=len(A)\n",
        "        nz = np.nonzero(A)\n",
        "    except:\n",
        "        return dgl.graph()\n",
        "    # forming the graph using the adjacency matrix\n",
        "    u1, v1 = list(nz[0]), list(nz[1])\n",
        "    # print(sorted(u1)==sorted(v1))\n",
        "    # print(u1)\n",
        "    # print(v1)\n",
        "    g = dgl.graph((u1, v1))\n",
        "    bg = dgl.to_bidirected(g)\n",
        "\n",
        "    # # atoms\n",
        "    atom_features_list = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        atom_features_list.append(atom_to_feature_vector(atom))\n",
        "    x = np.array(atom_features_list, dtype = np.int64)\n",
        "\n",
        "    \n",
        "    bg.ndata['node_feat'] = torch.FloatTensor(x)\n",
        "    bg = dgl.add_self_loop(bg)\n",
        "\n",
        "    # return graph \n",
        "    return bg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjGf9mIheOzM"
      },
      "source": [
        "help(SaltRemover)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38Qn4KfHXdwH"
      },
      "source": [
        "# g = smiles2graph(smiles[0])\n",
        "# g = dgl.add_self_loop(g)\n",
        "\n",
        "from dgl.data import DGLDataset\n",
        "class MoleculesDataset(DGLDataset):\n",
        "    def __init__(self):\n",
        "        super().__init__(name='molecules')\n",
        "\n",
        "    def process(self):\n",
        "        # drug_id, smiles --> lists that store the values of the two\n",
        "\n",
        "        self.graphs = []\n",
        "\n",
        "        for i in range(len(drug_id)):\n",
        "            g = smiles2graphNew(smiles[i])\n",
        "            self.graphs.append(g)\n",
        "\n",
        "        # n_graphs = len(self.graphs)\n",
        "        # n_train = int(n_graphs * 0.6)\n",
        "        # n_val = int(n_graphs * 0.2)\n",
        "        # self.train_mask = torch.zeros(n_graphs, dtype=torch.bool)\n",
        "        # val_mask = torch.zeros(n_graphs, dtype=torch.bool)\n",
        "        # test_mask = torch.zeros(n_graphs, dtype=torch.bool)\n",
        "\n",
        "        # train_mask[:n_train] = True\n",
        "        # val_mask[n_train:n_train + n_val] = True\n",
        "        # test_mask[n_train + n_val:] = True\n",
        "        \n",
        "        # self.graph.ndata['train_mask'] = train_mask\n",
        "        # self.graph.ndata['val_mask'] = val_mask\n",
        "        # self.graph.ndata['test_mask'] = test_mask\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.graphs[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "\n",
        "dataset = MoleculesDataset()\n",
        "print(len(dataset))\n",
        "graph = dataset[0]\n",
        "\n",
        "print(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHVHF7cMGFbb"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layersList = nn.ModuleList()\n",
        "        self.layersList.append(SAGEConv(in_feats, out_feats, 'gcn'))\n",
        "        self.layersList.append(SAGEConv(out_feats, out_feats, 'gcn'))\n",
        "        self.attention = Attention(maxnodes,maxnodes)\n",
        "        self.fc = nn.Linear( maxnodes*out_feats , 100)\n",
        "        self.fc_mu = nn.Linear( 100 , 30)\n",
        "        self.fc_var = nn.Linear( 100 , 30)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, g, feats):\n",
        "        temp = feats\n",
        "        for L in self.layersList:\n",
        "            temp = L(g,temp)\n",
        "            temp = self.relu(temp)\n",
        "        temp2 = temp\n",
        "        dim1 = temp.shape[0]\n",
        "        if maxnodes != dim1:\n",
        "            padder = torch.zeros(maxnodes-dim1,3)\n",
        "            temp = torch.cat([temp, padder], dim = 0)\n",
        "        temp = self.attention(temp.t()).t()\n",
        "        temp = torch.flatten(temp)\n",
        "        temp = self.relu(self.fc(temp))\n",
        "        mu = self.fc_mu(temp)\n",
        "        # var = torch.exp(self.fc_var(temp))\n",
        "        log_var = self.fc_var(temp)\n",
        "        # return temp2, temp, mu, log_var\n",
        "        return temp2, mu, log_var\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, in_feat,out_feat):\n",
        "        super().__init__()             \n",
        "        self.Q = nn.Linear(in_feat,out_feat) # Query\n",
        "        self.K = nn.Linear(in_feat,out_feat) # Key\n",
        "        self.V = nn.Linear(in_feat,out_feat) # Value\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        Q = self.Q(x)\n",
        "        K = self.K(x)\n",
        "        V = self.V(x)\n",
        "        d = K.shape[0] # dimension of key vector\n",
        "        QK_d = (Q @ K.T)/(d)**0.5\n",
        "        prob = self.softmax(QK_d)\n",
        "        attention = prob @ V\n",
        "        return attention\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layersList = nn.ModuleList()\n",
        "        self.layersList.append(SAGEConv(in_feats, out_feats, 'gcn'))\n",
        "        self.layersList.append(SAGEConv(out_feats, out_feats, 'gcn'))\n",
        "        self.fc1 = nn.Linear( 30, 100)\n",
        "        self.fc2 = nn.Linear( 100, maxnodes*in_feats)\n",
        "        # self.attention = Attention(maxnodes, maxnodes)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "\n",
        "    def forward(self, g, res, hidden):\n",
        "        temp = self.fc1(hidden)\n",
        "        temp = self.fc2(temp)\n",
        "        temp = torch.reshape(temp, (maxnodes, 3))\n",
        "        # temp = self.attention(temp.t()).t()\n",
        "        temp = temp[:res.shape[0]]\n",
        "        temp += res\n",
        "        for L in self.layersList:\n",
        "            temp = L(g,temp)\n",
        "            temp = self.relu(temp)\n",
        "        return temp\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats):\n",
        "        super(VariationalAutoEncoder, self).__init__()\n",
        "        self.encoder = Encoder(in_feats, out_feats)\n",
        "        self.decoder = Decoder(out_feats, in_feats)\n",
        "        self.batchNormLayer = nn.BatchNorm1d(out_feats)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        # sampling\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps * std + mu\n",
        "\n",
        "    def forward(self, g, inputs):\n",
        "        # temp, encoded, mu, log_var = self.encoder(g, inputs)\n",
        "        temp, mu, log_var = self.encoder(g, inputs)\n",
        "        hidden = self.reparameterize(mu, log_var)\n",
        "        temp = self.batchNormLayer(temp)\n",
        "        temp = self.decoder(g, temp, hidden)\n",
        "        return temp "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjRAjWx7GFbc"
      },
      "source": [
        "set_seed(0)\n",
        "# g = smiles2graph(smiles[0])\n",
        "# g = dgl.add_self_loop(g)\n",
        "# 155*9 --> 155*3 -> 100\n",
        "# x*9 --> x*3 --> maxnodes*3 --> 100 \n",
        "model = VariationalAutoEncoder(g.ndata['node_feat'].shape[1], 3)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# loss_fn = nn.KLDivLoss() + nn.MSELoss()\n",
        "loss_fn1 = nn.KLDivLoss() \n",
        "loss_fn2 = nn.MSELoss()\n",
        "\n",
        "all_logits = []\n",
        "for e in range(1000):\n",
        "    pred = model(g, g.ndata['node_feat'])\n",
        "    pred = F.log_softmax(pred, 1)\n",
        "    loss = loss_fn1(pred, g.ndata['node_feat']) + loss_fn2(pred, g.ndata['node_feat'])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if e % 50 == 0:\n",
        "        print('In epoch {}, loss: {}'.format(e, loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfvvkChxGFbd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}